---
title: "inClass-02-20-2018"
output: html_document
---

```{r}
library(MASS)
library(ISLR)
library(boot)
```

# questions

### what is the difference between validation set approach and cross validation?

### what is the difference between cross validation and LOOCV?

### Advantages of LOOCV over ordinary cross validation?

### Advantages of ordinary cross validation over LOOCV?

### What is the reason for the low bias of LOOCV? 

### What is the reason for the high variability of LOOCV?

### How would you implement a K-Fold Cross Validation function from scratch?



### Consider an ordinary quantitative regression problem
##### estimate residual error from formula
```{r}
fit.1 = lm(Sales ~ ., data = Carseats)
summary(fit.1)

# lets omit some of these categorical features
fit.2 = lm(Sales ~ . - Population - Education - Urban - US, data = Carseats)
summary(fit.2)
```

##### estimate residual error from training set/test set

```{r}
train = sample(400,300,replace = F)

fit.3 = lm(Sales ~ . - Population - Education - Urban - US, data = Carseats[train,])

summary(fit.3)

Sales.predicted = predict(fit.3, newdata = Carseats[-train,])

sqrt(mean((Sales.predicted - Carseats$Sales[-train])^2))
```

##### from LOOCV

```{r}

fit.1.glm = glm(Sales ~ . - Population - Education - Urban - US, data = Carseats)

fit.loocv <- cv.glm(Carseats, fit.1.glm)
fit.loocv$delta
```

##### from 5-fold cross validation
##### from 10-fold cross validation
```{r}
fit.cv10 <- cv.glm(Carseats, fit.1.glm, K = 10)
fit.cv10$delta

fit.cv5 <- cv.glm(Carseats, fit.1.glm, K = 5)
fit.cv5$delta
```

### Examine the variability of estimated residuals for k-fold approach vs validation set approach

## Validation set
```{r}
residuals <- c()
for (i in 1:30){
  train = sample(400,200,replace = F)
  
  fit.3 = lm(Sales ~ . - Population - Education - Urban - US, data = Carseats[train,])
  
  summary(fit.3)
  
  Sales.predicted = predict(fit.3, newdata = Carseats[-train,])
  
  residuals <- c(residuals, sqrt(mean((Sales.predicted - Carseats$Sales[-train])^2)))
  
}
boxplot(residuals)
```

## k-fold
```{r}
residuals <- c()
for (i in 1:30){
  fit.cv10 <- cv.glm(Carseats, fit.1.glm, K = 10)
  residuals <- c(residuals, fit.cv10$delta[1])
}
boxplot(residuals)
```


### use these methods to decide between two different models (linear or quadratic)
### how would this be done? 

### Decide between several different models using CV



### Make synthetic data
```{r}
mydf = data.frame(x = runif(100,min = -10, max = 10))
mydf$y = sin(mydf$x) + .1*mydf$x^2 + rnorm(100)
plot(y ~ x, data = mydf)
```

# Make a linear model and assess it with 10-fold CV.
```{r}
fit.x.1 <- glm(y ~ x, data = mydf)
fit.x.1.cv <- cv.glm(mydf, fit.x.1,K = 10)
fit.x.1.cv$delta

fit.x.2 <- glm(y ~ poly(x,2), data = mydf)
fit.x.2.cv <- cv.glm(mydf, fit.x.2,K = 10)
fit.x.2.cv$delta
```

# Exercise: Now do this for polynomial models up to order 10 and examine the error.



### Consider a classification problem

### Cross validation for mnist data 
```{r}
load("./mnist68.RData")
mnist68$target <- mnist68$labels == 8
```

# We shall use features  X463, X513, and maybe X753
```{r}
fit.2 <- glm(target ~ V463 + V513, data = mnist68, family = binomial)
pred.2 = predict(fit.2, mnist68, type = "response")

table(mnist68$target, pred.2 > .5)

fit.2cv5 <- cv.glm(mnist68, fit.2, K = 5)

names(fit.2cv5)

fit.2cv5$delta

fit.2cv10 <- cv.glm(mnist68, fit.2, K = 10)

fit.2cv10$delta

fit.2loocv <- cv.glm(mnist68, fit.2)

fit.2loocv$delta
```

# Exercise: Use CrossValidation to perform "Feature Selection". Try the model with one, two, or all three pixel features and calculate the 5-Fold CV Error for each model. 
