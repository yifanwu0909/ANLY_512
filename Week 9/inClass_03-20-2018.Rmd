---
title: "InClass 03/20/2018"
output:
  html_document:
    df_print: paged
---

```{r}
require("splines")
require("gam")
require("ISLR")
```

## Section 1: On polynomials & steps


Make a sequence of x values to build model matrices.
Also make a nonlinear response variable and store everything in a data frame.
```{r}
x = seq(-3,3,by=.01)
y = cos(x) + rnorm(length(x))/2
mydf = data.frame(x = x, y = y)
```

Make the model matrix for simple linear regression.

```{r}
X1 = model.matrix(y ~ x)
head(X1)
```

Make a model matrix for polynomial regression.
The basis functions are the powers of the X variable.
```{r}
X6 = model.matrix(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6))
```


Also make a model matrix using the poly() function.
This also makes basis functions which are polynomials of increasing degree.
However, these basis functions have better numerical and statistical properties as we will see shortly.

```{r}
X6p = model.matrix(y ~ poly(x,6))
head(X6)
head(X6p)
```

Make a plot of the basis functions x, x^2, ..., x^6.
Since these values are fairly large near the ends of the interval, we need a large ylim.

```{r}
plot(x,X6[,1], type = 'l', ylim = c(-50,50))
matlines(x,X6[,2:7],lwd = 2)
grid(col = 3)
```

Compute also the correlation coefficients.
As you can see, even powers are correlated with each other and odd powers are also correlated with each other.

```{r}
round(cor(X6[,2:7]),2)
```

By contrast, the basis function generated by poly() are not correlated at all.
They are "orthogonal polynomials".

```{r}
round(cor(X6p[,2:7]),2)
```


The basis functions x, x^2, ... also have less desirable numerical properties.
In the matrix formulation of linear regression, the following matrix and its inverse appear:

```{r}
A = t(X6)%*%X6
B = solve(A)

```

Then A*B should be the identity matrix.
Let's compute the product and check how this differs from the identity matrix:
```{r}
max(abs(A%*%B -diag(rep(1,7))))

```

This should really be much smaller. However, for this particular problem, it does not cause trouble.

Now plot the orthogonal basis functions generated by poly().

```{r}
plot(x,X6p[,1], type = 'l', ylim = c(-2,2))
matlines(x,X6p[,2:7],lwd = 2)
grid(col = 3)

```

All basis functions are much more similar in magnitude.
Zoom in:

```{r}
plot(x,0*x,ylim=c(-.2,.2), type = 'l')
matlines(x,X6p[,2:7],lwd = 2)
grid(col = 3)
```

These basis functions are  all uncorrelated. That is a consequence (and in fact essentially equivalent) to being orthogonal. 

```{r}
round(cor(X6p[,2:7]),2)

```

Their numerical properties are also better.

```{r}
A = t(X6p)%*%X6p
B = solve(A)
max(abs(A%*%B -diag(rep(1,7))))
```

This is much closer to zero.

## Step Functions.

Make a model matricx. The steps are set to be at the integers -2, -1, 0, 1, 2.

```{r}
Xstep = model.matrix(y ~ I(x > -2) + I(x > -1) + I(x > 0) + I(x > 1) + I(x > 2))
head(Xstep)

```

Look at the model matrix near x = -1. One of the step functions has a jump here.

```{r}
Xstep[195:205,]

```

Plot the functions and examine their correlation.

```{r}
plot(x,Xstep[,1], type = 'l', ylim = c(-2,2))
matlines(x,Xstep[,2:6],lwd=2)
grid(col = "darkgrey")
round(cor(Xstep[,2:6]),2)
```

These step functions are moderately correlated, but not as highly as pure powers.

## Fiting regression models to the data.

Then compute predictions, plot the original data, plot the "true" curve, plot the fitted model plus error bands.

```{r}
fit0 = lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6))
preds=predict(fit0,newdata=list(x=x),se=TRUE)
se.bands=cbind(preds$fit + 2*preds$se, preds$fit-2*preds$se)
plot(x,y,col="darkgrey")
lines(x,cos(x),lwd = 2, lty = 2)
lines(x,preds$fit,lwd=2,col="blue")
matlines(x,se.bands,col="blue",lty=2)
grid(col = "darkgrey")
```


Do the same thing with the model generated from orthogonal polynomials. Plot the fit and error bands in the same plot in red.

```{r}
plot(x,y,col="darkgrey")
lines(x,cos(x),lwd = 2, lty = 2)
lines(x,preds$fit,lwd=2,col="blue")
matlines(x,se.bands,col="blue",lty=2)
grid(col = "darkgrey")

fit1 = lm(y ~ poly(x,6))
preds1=predict(fit1,newdata=list(x=x),se=TRUE)
se.bands1=cbind(preds1$fit + 2*preds1$se, preds1$fit-2*preds1$se)
lines(x,preds1$fit,lwd=2,col="red")
matlines(x,se.bands1,col="red",lty=2)

```

As you can see, the two predictions agree completely. That is because both predictions are based on polynomial models of degree up to six.
The two fitted polynomials are exactly the same, they are just written differently. One is a linear combination of pure powers and the other is a linear combination of orthogonal polynomials, with these coefficients:
```{r}
fit0$coef
fit1$coef

```

As you can see, the coefficients in the first set become smaller and smaller.
The coefficients in the second set all have similar magnitudes. 
 
Just for fun, fit a 12th degree polynomial: 

 ```{r}
fit1b = lm(y ~ poly(x,12))
fit1b$coef

```

The first seven coefficients are the same as those for the 6th degree polynomial.
That happens because the polynomials are orthogonal.

When the fit from the 12th degree polynomial model is plotted, there are more wiggles, in particular near the ends of the interval.

```{r}
plot(x,y,col="darkgrey")
lines(x,cos(x),lwd = 2, lty = 2)
lines(x,preds$fit,lwd=2,col="blue")
matlines(x,se.bands,col="blue",lty=2)
grid(col = "darkgrey")

preds1b=predict(fit1b,newdata=list(x=x),se=TRUE)
se.bands1b=cbind(preds1b$fit + 2*preds1b$se, preds1b$fit-2*preds1b$se)
lines(x,preds1b$fit,lwd=2,col="magenta")
matlines(x,se.bands1b,col="magenta",lty=2)
```

Redo the plot,with  the 6th degree polynomial and the fit from the step function:
```{r}
plot(x,y,col="darkgrey")
lines(x,cos(x),lwd = 2, lty = 2)
lines(x,preds$fit,lwd=2,col="blue")
matlines(x,se.bands,col="blue",lty=2)
grid(col = "darkgrey")

fit2 = lm(y ~ I(x > -2) + I(x > -1) + I(x > 0) + I(x > 1) + I(x > 2))
preds2=predict(fit2,newdata=list(x=x),se=TRUE)
se.bands2=cbind(preds2$fit + 2*preds2$se, preds2$fit-2*preds2$se)
lines(x,preds2$fit,lwd=2,col="magenta")
matlines(x,se.bands2,col="magenta",lty=2)

```

The step function model clearly has worse performance.

 
##Section 2: Cubic splines and Smoothing Splines
Cubic spline with fixed knots.

```{r}
agelims=range(Wage$age)
age.grid=seq(from=agelims[1],to=agelims[2])

spline_fit <- lm(wage ~ bs(age, knots=c(20,40,60)), data=Wage)
plot(Wage$age, Wage$wage, pch=19, col='grey')
pred = predict(spline_fit,list(age=age.grid), se=T)
lines(age.grid,pred$fit, col="#3690C0",lwd=4)
```
 A natural spline has better behavior at the boundary points.

```{r}
agelims=range(Wage$age)
age.grid=seq(from=agelims[1],to=agelims[2])

spline_fit <- lm(wage ~ ns(age, knots=c(20,40,60)), data=Wage)
plot(Wage$age, Wage$wage, pch=19, col='grey')
pred = predict(spline_fit,list(age=age.grid), se=T)
lines(age.grid,pred$fit, col="#3690C0",lwd=4)
```


A smoothing spline is a cubic spline with a knot at every observed $x$ but also a penalization to encourage smoothness.

```{r}
plot(Wage$age, Wage$wage, pch=19, col='grey')
ss_fit <- smooth.spline(Wage$age,Wage$wage,cv=TRUE)
lines(ss_fit,col="firebrick", lwd=4)
```

#### S&P Data
We will working with the weekly S&P500 data.

```{r}
names(Weekly)
head(Weekly)
attach(Weekly)
```
We want to predict weekly Volume from performance in previous weeks. (This make little sense, so this is just a formal exercise.)
Make an additive model that uses polynomial regression for each of the predictors.

```{r}
fit4 = gam(Volume ~ poly(Lag1,3 ) + poly(Lag2,3 ) + poly(Lag3,3 ))
summary(fit4)
```

Plot the data and the fits. 

```{r}
plot(Volume,col="darkgrey")
preds.Weekly=predict(fit4,se=TRUE)
lines(preds.Weekly$fit,lwd=2,col="blue")
```

Clearly the fits have nothing to do with the data. It is essential to include time in the fit. Also, the exponential increase of the trading volume suggests that one should loogk at the log of the volume.

```{r}
plot(log10(Volume), col = "darkgrey")

```

There is a general linear trend, the magnitude of the variation is about the same for all years, there are deviations from the linear trend around the years 2002 and 2008, and there are also seasonal variation. 

## Smoothing splines

 We can use smoothing splines to summarize these data. For very large penalty parameters $\lambda$ or equivalently $df = 2$, a smoothing spline is essentially a straight line. Make such a smoothing spline, compute predictions, and plot it. Choosing a large lambda is equivalent to using two degrees of freedom (two parameters are fitted).

```{r}
fit.s1 = smooth.spline(log10(Volume) ~  1:1089, df = 2)
preds.s1 = predict(fit.s1)
plot(log10(Volume), col = "darkgrey")
lines(preds.s1$y, lwd =2 )
```

To capture year-to-year variation, increase the number of degrees of freedom. We use one df per year, plus one for the intercept. This is plotted in red.

```{r}
fit.s1 = smooth.spline(log10(Volume) ~  1:1089, df = 2)
preds.s1 = predict(fit.s1)
fit.s22 = smooth.spline(log10(Volume) ~  1:1089, df = 22)
preds.s22 = predict(fit.s22)

plot(log10(Volume), col = "darkgrey")
lines(preds.s1$y, lwd =2 )
lines(preds.s22$y, lwd =2, col = 2)

```

To capture also seasonal variation, increase the number of degrees of freedom further. We use four df per year, plus one for the intercept. This is plotted in blue.

```{r}
fit.s85 = smooth.spline(log10(Volume) ~  1:1089, df = 85)
preds.s85 = predict(fit.s85)

plot(log10(Volume), col = "darkgrey")
lines(preds.s85$y, lwd =2, col = 4)
```

When the number of degrees of freedom is not specified, \texttt{gam} chooses one. Here is the resulting plot. This fit uses about 120 degrees of freedom.
```{r}
fit.s = smooth.spline(log10(Volume) ~  1:1089)
preds.s = predict(fit.s)

plot(log10(Volume), col = "darkgrey")
lines(preds.s$y, lwd =2, col = 4)
fit.s
```

### Exercise: GAMs
1. Use strictly linear model to predict Salary from Age, Year, and Education. What's the total RSS of this model?

2. Use the function gam() to predict Salary from Age, Year and Education. For hich of these predictors should you leverage smoothing splines? Use a low degree of freedom for these (such as df=1 or df=2). Make a plot of the resulting gam-fit object, what relationships are observed between the predictors and the target varible?  What's the total RSS of this model?

```{r}
gam_fit <- gam(wage ~ s(age,df=1) + s(year, df=1) + education ,data=Wage)
par(mfrow=c(1,3))
plot(gam_fit,se=T)
t(gam_fit$residuals) %*% gam_fit$residuals
```
3. Use nnet to predict Salary from Age, Year and Education. What's the total RSS of this model? How does nnet interpretability compare to GAM interpretability? 

```{r}
library(nnet)
nnet_fit <- nnet(wage ~ age + year + education, size=25, data=Wage, decay=.01)
t(nnet_fit$residuals) %*% nnet_fit$residuals
```