---
title: "inClass-02/27/2018"
output: html_document
---

```{r}
library(ISLR)
library(leaps)
```

## Subset Selection

### Best Subset

If we have a data set with $p$ predictors, the total number of possible linear models we could attempt is $2^p$. Why is that?

With Best Subset selection, we do a brute-force search and evaluate all $2^p$ models and look for the best one (see Figure 6.1 in the book). Why is it important that we only make comparisons between models that have the same number of predictors? Why can we just choose the best model from the set of the $2^p$ models we evaluated?

*Exercise*
Try Best Subset with the Hitters data using the regsubsets function in the leaps package. 

### Forward Stepwise

For computational savings, we can turn to Forward Stepwise which doesn't compute $2^p$ models but a much smaller set. Forward Stepwise proceeds through greedy search. How does Forward Stepwise proceed? What are the limitations?

**Exercise**
Try Forward Stepwise with the Hitters data. 

### Backward Stepwise




## Model Comparison Metrics

All of the following metrics are based on the idea of trading off a model's goodness-of-fit with the model's flexibility. That is, they are all a quantification of how we should _penalize_ models with more parameters. Their only major differences are how they quantify this penalty. 

The following formula apply specifically to the case of linear regression. Therefore, the goodness-of-fit metric is the RSS. But these metrics have a more generic form which can be applied to any statistical model, and where the goodness-of-fit is quantified by the maxkimum of the Likelihood ($\hat{L}$).

### Mallow's C
\[
C_p = \frac{1}{n}(RSS + 2d\hat{\sigma}^2)
\]


### Akaike Information Criterion
\[
AIC = \frac{1}{n \hat{\sigma}^2}(RSS + 2d\hat{\sigma}^2)
\]
 or more generally

\[
AIC = 2d - 2ln(\hat{L})
\]

### Bayesian Information Criterion

\[
BIC = \frac{1}{n \hat{\sigma}^2}(RSS + log(n)d\hat{\sigma}^2)
\]

or more generally 

\[
BIC = ln(n)d - 2ln(\hat{L})
\]

### Adjusted R Squared

\[
Adj R^2 = 1 - \frac{RSS/(n-d-1)}{TSS/(n-1)}
\]


**Exercise**
Let's return to the Hitters data and Best Subset selection. From the outputs of regsubsets, we can obtain metrics like BIC for each sub-model with K predictors.

```{r, eval=F}
regsubsets(Salary ~ ., data=Hitters, nvmax = 19, method= c("forward"))
full_subsets_summary = summary(full_subsets)
names(full_subsets_summary)
```

Make a few plots showing BIC, and Mallow's C, and 1-AdjustedRsquared. Are the different metrics in rough agreement?

Question: These metrics are evaluted with the full training set. Another way we can approach the problem of model selection is to use cross validation. What are the pros and cons of each approach?