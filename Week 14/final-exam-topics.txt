Topics for Final Exam

The Final will cover sections 2.1-2, 3.1-3, 4.1-3, 4.5, 5.1-2, 6.1-2, 6.4, 7.1-5, 8.1-2, 9.1-3, 10.1-3, and material on artificial neural networks. 
Closed notes, closed books. 
Calculator use will be allowed during the final exam.

You should be familiar with the following definitions and concepts (recognize them, give examples, apply them where appropriate):

    Basic concepts of statistical learning:
        Prediction and Inference
        accuracy and interpretability
        regression and classification
        supervised and unsupervised learning
        Bias-variance tradeoff
    K-nearest neighbors
    Simple and multiple linear regression
    Simple and multiple logistic regression
    Model matrix
    Confusion matrix
    Artificial neural networks (ANN)
    Input layers, output layers, hidden layers, weights for ANNs
    ROC curve, sensitivity, specificity
    Training sets, test sets, cross validation, including leave-one-out CV
    Subset selection (best, stepwise forward and backward)
    Ridge regression, Lasso
    Polynomial regression, regression splines, smoothing splines
    Decision trees, bagging, random forests, boosting, pruning a tree
    Maximum margin classifier, support vector classifiers and support vector machines
    Principal components, biplots
    K-means clustering
    Hierarchical clustering, linkage

You should be able to do the following:

    Recognize regression and classification problems and recommend methods
    Recognize unsupervised and supervised learning situations
    Give interpretation of R output for linear regression
    Give interpretation of R output for ANNs, including drawing and labeling networks
    Sketch an ANN for a proposed network topology
    Give interpretation of ROC curve
    Give interpretation of R output for cross-validation
    Interpret graphical output of subset selection techniques (BIC, AIC, Cp) and make recommendations
    Interpret output of ridge regression and make recommendations
    Interpret output of Lasso and make recommendations
    Interpret output of polynomial regression, make recommendations
    Set up and interpret model matrices
    Interpret output of smoothing splines
    Build simple decision trees for regression and classification
    Draw a decision tree from a partition of the feature space, partition the feature space using a decision tree 
    Interpret and compare output from bagging, boosting, random forests
    Interpret output from support vector machines, make recommendations
    Interpret PCA output and biplots
    Identify possible tuning parameters for all these methods
    Demonstrate steps of K-means clustering steps and hierarchical clustering

The following topics will not be on the exam:

    Fitting regression models by hand
    Computing K-nearest neighbor predictions
    Computing points on a ROC curve by hand
    Writing R code, checking R syntax
    Leverage, hat-matrix
    Linear and quadratic discriminant analysis
    Dummy variables
    Diagnostic plots for regression
    Kappa statistic
    Linear models based on principal components regression, partial least squares
    Back propagation for fitting ANNs
    Set up model matrices with step functions or regression splines
    Local regression
    Generalized additive models
    Prune decision trees by hand
    Perform boosting by hand
    Construct maximum margin classifiers, support vector classifiers, or support vector machines by hand
    SVMs with more than two classes
    Compute principal components by hand
