---
title: "inClass-02072016"
output:
  html_document:
    df_print: paged
---

## 1. Logistic Regression
 * Logit Function
 * odds ratio, log odds
 * coefficient intepretation
 
## 2. Mulitple Logistic Regression
  * parameter interpretation
  * Interactions
 
## 3. Interactive Application
  - explore the accompanying Shiny App
  
$p(Y=1|X) = \beta_0 + \beta_1 Age + \beta_2 Female + \beta_3 Age*Female$
  
*Exercise*
1. What is the effect on the sigmoid curve of changing beta_0?
2. What is the effect of changing beta_1?
3. What is the effect of changing beta_2?
4. What is the effect of the interaction term beta_3? (it's a little hard to tell)
5. Can you find a setting of the sliders such that the relationship between Y and Age is reversed?



## 4. Model Evaluation in Classification
 * Prediction accuracy
 * True Positives, False Positives, True Negatives, False Negatives
 * Confusion Matrix
 * ROC curve, AUC
 
```{r}
example2 <- read.csv("./example1.csv", header=TRUE)
names(example2) <- c("X","Y","Z")

plot(Y ~ X, data = example2, col = Z+1, lwd = 3, asp = 1)
```
 
Now let's fit a logistic regression model and make a confusion matrix.

```{r}
fit <- glm(Z ~ X + Y , data = example2, family=binomial)
example2$pred = predict(fit, example2, type = "response")
table(example2$Z, example2$pred > .5)
```
 
What happens when we change the threshold?


```{r}
table(example2$Z, example2$pred > .8)
```
 
 

```{r}
table(example2$Z, example2$pred > .1)
```

 
*Exercise* - We're going to make our own ROC curve

1. Create a DataFrame for evaluation data where you have column corresponding to the "true" labels and a column corresponding to the predicted probabilities.

2. Create a function that takes in a DataFrame (like the above one) as well as a probability threshold, and returns the corresponding confusion matrix.

3. Simplify your previous function by having it return only the True Positive Rate and number of False Positive Rate.

```{r}

true_pos <- function(df,thre){
  conf_mtrx <- as.data.frame.matrix(table(example2$Z, example2$pred > thre))
  a <- conf_mtrx["1", "TRUE"]
  b <- conf_mtrx["0", "TRUE"]
  return(paste("When threshold is", thre, ", the True positive is:", a, ", and False positive is:", b, "."))
}

true_pos(example2, 0.8)
true_pos(example2, 0.1)

```

4. For a range of probability thresholds from 0.0 to 1.0, evaluate you function from step 3 and collect the corresponding True Positive Rates and False Positive Rates.

5. Make a plot of TPR vs FPR (FPR on x-axis).

```{r}
library(pROC)
r <- roc(example2$Z, example2$pred)
plot(r)
auc(example2$Z, example2$pred)
```
 
# Exercise

##  MNIST

The MNIST dataset is a famous benchmark in computer vision. It consists of low-resolution images of handwritten digits (from postal letters). I've prepared a subset of MNIST consisting of just images of "6" and of "8". This dataframe contains nearly twelve thousand images. 

```{r}
load("./mnist68.RData")
dim(mnist68)

head(mnist68$labels)
```
Each image is 28 pixels by 28 pixels. The scalar value of each pixel just refers to how light or dark each pixel is: a white pixel is a value of 0, a black pixel is a value of 1, and everything in between is some shade of gray. 

In this dataframe, each row corresponds to a single image. The first 784 columns are the pixels and their corresponding values. The final column is the target label. Let's take a look at some examples.

```{r}
plot_digit <- function(j){
  arr784 <- as.numeric(mnist68[j,1:784])
  col=gray(12:1/12)
  image(matrix(arr784, nrow=28)[,28:1], col=col, 
        main = paste("this is a ",mnist68$labels[j]))
}
```

```{r}
plot_digit(9594)
```


```{r}
plot_digit(9591)
```


This data is obviously quite high dimensional compared to some of the datasets we looked at in the ISLR package. But this high-dimensionality is more realistic for contemporary problems that involve images, audio, video, and natural language. 

We are going to build a classifier to predict whether an image is of an "8" or a "6".

1. There are 784 predictors in these images. Pick a feature at random (ie. a column of the MNIST dataframe). Compute the standard deviation of this feature.

2. We want to find features (pixels) that aren't always white for every image. We need to find some with non-zero variance if we hope to have any chance at building a classifier. Identify two such features from the dataframe.


3. Create a new dataframe with your two features plus the label column. Make a plot of feature_1 vs feature_2 and color the points according to the label. 

4. Run a logistic regression on this model. How does the model perform?

5. Find a few more features(pixels) that you can add in to the model and run again. 


```{r}
sd(mnist68$V75)

non-zero <- subset(mnist68, sex=="m" & age > 25, select=weight:income)

```



## 2.  Auto
```{r}
library(ISLR)
head(Auto)
```
(a) Make a binary variable mpg01 that contains a 1 if mpg is above its median, and 0 otherwise.

(b) Graphical investigation. We can make scatter plots of two numeric variables, with mpg01 as color.

(c) Fit a Logistic Regression model with 2 of the predictors. Interpret the coefficients.

(d) logistic regression on the training data. Make a confusion matrix for the Bayes classifier (cutoff = p = 0.5)

(e) Make a ROC curve.


```{r}
library(plyr)
mydata<-mtcars
var.68 = colwise(var)(mydata)
var.68
```



























